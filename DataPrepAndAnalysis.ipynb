{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import hmean\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, concatenate, Activation\n",
    "from keras.layers import Flatten, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Train Dataset\n",
    "tweets=pd.read_csv(\"https://raw.githubusercontent.com/ashsin19/TwitterSentiment_DeepLearning/master/Data/train_data.csv?token=ACPGR5ZIEVEBVVLGHPIGEBK5KKNM2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>If Google maps can't keep up with road constru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Autonomous cars seem way overhyped given the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Just saw Google self-driving car on I-34. It w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  Two places I'd invest all my money if I could:...\n",
       "1          1  Awesome! Google driverless cars will help the ...\n",
       "2          0  If Google maps can't keep up with road constru...\n",
       "3          0  Autonomous cars seem way overhyped given the t...\n",
       "4          2  Just saw Google self-driving car on I-34. It w..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "tok = WordPunctTokenizer()\n",
    "#stop_words = set(stopwords.words('english')) \n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "pat3 = r'RT @[A-Za-z0-9_]+'\n",
    "pat4 = r'RT @[A-Za-z0-9_ ] RT+'\n",
    "#combined_pat = r'|'.join((pat1, pat2,pat3))\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "rt_pattern= re.compile(pat3,flags=re.IGNORECASE)\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    if rt_pattern.match(bom_removed):\n",
    "        rt_stripped=re.sub(pat3,'',bom_removed)\n",
    "        stripped = re.sub(combined_pat, '', rt_stripped)\n",
    "    else:\n",
    "        stripped = re.sub(combined_pat, '', bom_removed)\n",
    "        stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    #tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Tweets 525 of 5250 has been processed\n",
      "Tweets 1050 of 5250 has been processed\n",
      "Tweets 1575 of 5250 has been processed\n",
      "Tweets 2100 of 5250 has been processed\n",
      "Tweets 2625 of 5250 has been processed\n",
      "Tweets 3150 of 5250 has been processed\n",
      "Tweets 3675 of 5250 has been processed\n",
      "Tweets 4200 of 5250 has been processed\n",
      "Tweets 4725 of 5250 has been processed\n",
      "Tweets 5250 of 5250 has been processed\n"
     ]
    }
   ],
   "source": [
    "x = len(tweets.index)\n",
    "nums = [0,x]\n",
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    if( (i+1)%525 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                    \n",
    "    clean_tweet_texts.append(tweet_cleaner_updated(tweets['text'][i]))\n",
    "    clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['target'] = tweets.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clean_df.text\n",
    "y = clean_df.target\n",
    "SEED = 5\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2673639.63it/s]\n",
      "W0813 12:29:29.145951 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1315810.94it/s]\n",
      "W0813 12:29:29.317490 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2573944.59it/s]\n",
      "W0813 12:29:29.483047 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 12:29:29.645612 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632093.71it/s]\n",
      "W0813 12:29:29.832113 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 12:29:30.003655 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5265446.20it/s]\n",
      "W0813 12:29:30.172727 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2634297.88it/s]\n",
      "W0813 12:29:30.370866 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5135283.58it/s]\n",
      "W0813 12:29:30.536423 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5267965.55it/s]\n",
      "W0813 12:29:30.706966 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2634613.07it/s]\n",
      "W0813 12:29:30.880503 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 12:29:31.048054 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 12:29:31.218598 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5262929.25it/s]\n",
      "W0813 12:29:31.386150 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5265446.20it/s]\n",
      "W0813 12:29:31.548716 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5267965.55it/s]\n",
      "W0813 12:29:31.716267 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 12:29:31.885098 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2588772.16it/s]\n",
      "W0813 12:29:32.050845 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5267965.55it/s]\n",
      "W0813 12:29:32.220392 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5264187.43it/s]\n",
      "W0813 12:29:32.389938 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 12:29:32.556492 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5269226.13it/s]\n",
      "W0813 12:29:32.724044 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5270487.31it/s]\n",
      "W0813 12:29:32.897580 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5094885.70it/s]\n",
      "W0813 12:29:33.064135 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2599161.47it/s]\n",
      "W0813 12:29:33.230851 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 12:29:33.396408 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2592734.72it/s]\n",
      "W0813 12:29:33.563697 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2598241.42it/s]\n",
      "W0813 12:29:33.733925 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 12:29:33.902475 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 12:29:34.071024 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6874205844980941"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pure Distributed Bag of Words\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dbow.alpha -= 0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha\n",
    "    \n",
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "    return vecs\n",
    "  \n",
    "train_vecs_dbow = get_vectors(model_ug_dbow, x_train, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, x_validation, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow, y_train)\n",
    "clf.score(validation_vecs_dbow, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754449.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 12:29:40.712861 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 12:29:40.956767 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5132889.51it/s]\n",
      "W0813 12:29:41.283454 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 12:29:41.609169 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5269226.13it/s]\n",
      "W0813 12:29:41.861494 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 877539.39it/s]\n",
      "W0813 12:29:42.189618 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1316204.18it/s]\n",
      "W0813 12:29:42.511755 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2599468.30it/s]\n",
      "W0813 12:29:42.795528 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754729.14it/s]\n",
      "W0813 12:29:43.079273 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5265446.20it/s]\n",
      "W0813 12:29:43.409389 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1053038.88it/s]\n",
      "W0813 12:29:43.708590 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 12:29:43.986846 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5270487.31it/s]\n",
      "W0813 12:29:44.320495 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 12:29:44.588779 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5265446.20it/s]\n",
      "W0813 12:29:44.895959 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1753890.56it/s]\n",
      "W0813 12:29:45.215104 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633352.79it/s]\n",
      "W0813 12:29:45.517877 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 12:29:45.832833 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 12:29:46.118071 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5262929.25it/s]\n",
      "W0813 12:29:46.404306 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 12:29:46.675580 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2598548.03it/s]\n",
      "W0813 12:29:46.949924 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2536877.42it/s]\n",
      "W0813 12:29:47.246263 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 12:29:47.489612 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5256647.41it/s]\n",
      "W0813 12:29:47.757894 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5140078.43it/s]\n",
      "W0813 12:29:48.040140 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 12:29:48.281494 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 12:29:48.536875 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5135283.58it/s]\n",
      "W0813 12:29:48.780912 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 12:29:49.060166 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6493011435832274"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pure Distributed Memory Mean\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmm = Doc2Vec(dm=1, dm_mean=1, size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmm.alpha -= 0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha\n",
    "    \n",
    "train_vecs_dmm = get_vectors(model_ug_dmm, x_train, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, x_validation, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, y_train)\n",
    "clf.score(validation_vecs_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6721728081321474"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DBOW+DMM\n",
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_train, 200)\n",
    "validation_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_validation, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm, y_train)\n",
    "clf.score(validation_vecs_dbow_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1053240.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#Separate Word2Vec\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2,\n",
    "                         workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1753750.88it/s]\n",
      "W0813 12:30:04.774454 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1315732.31it/s]\n",
      "W0813 12:30:04.831303 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754868.98it/s]\n",
      "W0813 12:30:04.872469 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633667.74it/s]\n",
      "W0813 12:30:04.921340 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1755008.85it/s]\n",
      "W0813 12:30:04.964730 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 12:30:05.006619 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754309.75it/s]\n",
      "W0813 12:30:05.050012 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5259158.35it/s]\n",
      "W0813 12:30:05.092897 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1753331.95it/s]\n",
      "W0813 12:30:05.136780 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 12:30:05.178669 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5265446.20it/s]\n",
      "W0813 12:30:05.220556 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1755148.73it/s]\n",
      "W0813 12:30:05.263441 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754309.75it/s]\n",
      "W0813 12:30:05.306327 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 12:30:05.351206 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632093.71it/s]\n",
      "W0813 12:30:05.395089 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 12:30:05.440965 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 12:30:05.489836 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631150.20it/s]\n",
      "W0813 12:30:05.532720 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2630521.56it/s]\n",
      "W0813 12:30:05.579597 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631150.20it/s]\n",
      "W0813 12:30:05.623479 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 12:30:05.667361 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 12:30:05.708251 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2630521.56it/s]\n",
      "W0813 12:30:05.751137 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 12:30:05.801002 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2630835.84it/s]\n",
      "W0813 12:30:05.843889 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2635243.66it/s]\n",
      "W0813 12:30:05.886775 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 12:30:05.935677 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5265446.20it/s]\n",
      "W0813 12:30:05.982519 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632093.71it/s]\n",
      "W0813 12:30:06.028396 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2591514.18it/s]\n",
      "W0813 12:30:06.073281 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754309.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1315732.31it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 12:30:06.203932 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2588772.16it/s]\n",
      "W0813 12:30:06.258788 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1753890.56it/s]\n",
      "W0813 12:30:06.309652 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1315810.94it/s]\n",
      "W0813 12:30:06.364505 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 12:30:06.419359 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633352.79it/s]\n",
      "W0813 12:30:06.470731 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 12:30:06.528573 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2642517.22it/s]\n",
      "W0813 12:30:06.580947 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633667.74it/s]\n",
      "W0813 12:30:06.632804 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633667.74it/s]\n",
      "W0813 12:30:06.683667 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631150.20it/s]\n",
      "W0813 12:30:06.734531 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5267965.55it/s]\n",
      "W0813 12:30:06.788391 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 12:30:06.844238 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632093.71it/s]\n",
      "W0813 12:30:06.898096 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754309.75it/s]\n",
      "W0813 12:30:06.952946 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2630521.56it/s]\n",
      "W0813 12:30:07.007801 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1755008.85it/s]\n",
      "W0813 12:30:07.061656 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2630835.84it/s]\n",
      "W0813 12:30:07.113519 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2630521.56it/s]\n",
      "W0813 12:30:07.162388 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 12:30:07.216244 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 12:30:07.271095 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 12:30:07.325950 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2626129.52it/s]\n",
      "W0813 12:30:07.376814 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633667.74it/s]\n",
      "W0813 12:30:07.425683 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2597628.41it/s]\n",
      "W0813 12:30:07.477053 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1757670.50it/s]\n",
      "W0813 12:30:07.536895 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 12:30:07.583774 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5267965.55it/s]\n",
      "W0813 12:30:07.632643 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 12:30:07.683507 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2628637.46it/s]\n",
      "W0813 12:30:07.737363 13340 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6556543837357052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6683608640406608\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]),\n",
    "                        total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_cbow.alpha -= 0.002\n",
    "    model_ug_cbow.min_alpha = model_ug_cbow.alpha\n",
    "\n",
    "model_ug_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2,\n",
    "                       workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]),\n",
    "                      total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_sg.alpha -= 0.002\n",
    "    model_ug_sg.min_alpha = model_ug_sg.alpha\n",
    "\n",
    "def get_w2v_mean(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_cbow[word],model_ug_sg[word]).reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "  \n",
    "def get_w2v_sum(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_cbow[word],model_ug_sg[word]).reshape((1, size))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return vec\n",
    "  \n",
    "train_vecs_cbowsg_mean = scale(np.concatenate([get_w2v_mean(z, 200) for z in x_train]))\n",
    "validation_vecs_cbowsg_mean = scale(np.concatenate([get_w2v_mean(z, 200) for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_cbowsg_mean, y_train)\n",
    "print(clf.score(validation_vecs_cbowsg_mean, y_validation))\n",
    "\n",
    "train_vecs_cbowsg_sum = scale(np.concatenate([get_w2v_sum(z, 200) for z in x_train]))\n",
    "validation_vecs_cbowsg_sum = scale(np.concatenate([get_w2v_sum(z, 200) for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_cbowsg_sum, y_train)\n",
    "print(clf.score(validation_vecs_cbowsg_sum, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v general\n",
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6696315120711563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.662007623888183\n"
     ]
    }
   ],
   "source": [
    "#Word vectors extracted from Doc2Vec models with custom weighting\n",
    "cvec = CountVectorizer(max_features=100000)\n",
    "cvec.fit(x_train)\n",
    "\n",
    "neg_train = x_train[y_train == 0]\n",
    "pos_train = x_train[y_train == 1]\n",
    "neu_train = x_train[y_train == 2]\n",
    "neg_doc_matrix = cvec.transform(neg_train)\n",
    "pos_doc_matrix = cvec.transform(pos_train)\n",
    "neu_doc_matrix = cvec.transform(neu_train)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neu_tf = np.sum(neu_doc_matrix,axis=0)\n",
    "\n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "neu = np.squeeze(np.asarray(neu_tf))\n",
    "term_freq_df2 = pd.DataFrame([neg,pos,neu],columns=cvec.get_feature_names()).transpose()\n",
    "term_freq_df2.columns = ['negative', 'positive','neutral']\n",
    "term_freq_df2['total'] = term_freq_df2['negative'] + term_freq_df2['positive'] + term_freq_df2['neutral']\n",
    "term_freq_df2['pos_rate'] = term_freq_df2['positive'] * 1./term_freq_df2['total']\n",
    "term_freq_df2['pos_freq_pct'] = term_freq_df2['positive'] * 1./term_freq_df2['positive'].sum()\n",
    "term_freq_df2['pos_rate_normcdf'] = normcdf(term_freq_df2['pos_rate'])\n",
    "term_freq_df2['pos_freq_pct_normcdf'] = normcdf(term_freq_df2['pos_freq_pct'])\n",
    "term_freq_df2['pos_normcdf_hmean'] = hmean([term_freq_df2['pos_rate_normcdf'], term_freq_df2['pos_freq_pct_normcdf']])\n",
    "pos_hmean = term_freq_df2.pos_normcdf_hmean\n",
    "\n",
    "w2v_pos_hmean = {}\n",
    "for w in model_ug_dbow.wv.vocab.keys():\n",
    "    if w in pos_hmean.keys():\n",
    "        w2v_pos_hmean[w] = np.append(model_ug_dbow[w],model_ug_dmm[w]) * pos_hmean[w]\n",
    "        \n",
    "train_vecs_w2v_poshmean_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean, 'mean') for z in x_train]))\n",
    "validation_vecs_w2v_poshmean_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean, 'mean') for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_mean, y_train)\n",
    "print(clf.score(validation_vecs_w2v_poshmean_mean, y_validation))\n",
    "\n",
    "train_vecs_w2v_poshmean_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean, 'sum') for z in x_train]))\n",
    "validation_vecs_w2v_poshmean_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean, 'sum') for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_sum, y_train)\n",
    "print(clf.score(validation_vecs_w2v_poshmean_sum, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6797966963151207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6797966963151207\n"
     ]
    }
   ],
   "source": [
    "#Separately trained Word2Vec with custom weighting (Average/Sum)\n",
    "w2v_pos_hmean_01 = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    if w in pos_hmean.keys():\n",
    "        w2v_pos_hmean_01[w] = np.append(model_ug_cbow[w],model_ug_sg[w]) * pos_hmean[w]\n",
    "        \n",
    "train_vecs_w2v_poshmean_mean_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'mean') for z in x_train]))\n",
    "validation_vecs_w2v_poshmean_mean_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'mean') for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_mean_01, y_train)\n",
    "print(clf.score(validation_vecs_w2v_poshmean_mean_01, y_validation))\n",
    "\n",
    "train_vecs_w2v_poshmean_sum_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'sum') for z in x_train]))\n",
    "validation_vecs_w2v_poshmean_sum_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'sum') for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_sum_01, y_train)\n",
    "print(clf.score(validation_vecs_w2v_poshmean_sum_01, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow.save(\"w2v_model_ug_cbow.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_sg.save(\"w2v_model_ug_sg.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_ug_cbow = KeyedVectors.load('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=45)\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3675 samples, validate on 787 samples\n",
      "Epoch 1/5\n",
      " - 15s - loss: 0.9380 - acc: 0.6079 - val_loss: 0.8337 - val_acc: 0.6595\n",
      "Epoch 2/5\n",
      " - 14s - loss: 0.6153 - acc: 0.7592 - val_loss: 0.7638 - val_acc: 0.6811\n",
      "Epoch 3/5\n",
      " - 14s - loss: 0.2126 - acc: 0.9439 - val_loss: 0.8458 - val_acc: 0.6633\n",
      "Epoch 4/5\n",
      " - 14s - loss: 0.0765 - acc: 0.9820 - val_loss: 0.9219 - val_acc: 0.6480\n",
      "Epoch 5/5\n",
      " - 14s - loss: 0.0426 - acc: 0.9888 - val_loss: 1.0118 - val_acc: 0.6506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2653a2ac048>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ptw2v = Sequential()\n",
    "e = Embedding(100000, 200, input_length=45)\n",
    "model_ptw2v.add(e)\n",
    "model_ptw2v.add(Flatten())\n",
    "model_ptw2v.add(Dense(512, activation='relu'))\n",
    "model_ptw2v.add(Dense(10, activation='softmax'))\n",
    "model_ptw2v.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 45, 200)      20000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 44, 100)      40100       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 43, 100)      60100       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 42, 100)      80100       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 100)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 100)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 100)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 300)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          154112      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 10)           5130        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10)           0           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,339,542\n",
      "Trainable params: 20,339,542\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tweet_input = Input(shape=(45,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(100000, 200, input_length=45)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(512, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(10)(merged)\n",
    "output = Activation('softmax')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3675 samples, validate on 787 samples\n",
      "Epoch 1/5\n",
      "3675/3675 [==============================] - ETA: 1:08 - loss: 2.2927 - acc: 0.078 - ETA: 40s - loss: 2.2243 - acc: 0.359 - ETA: 30s - loss: 2.1497 - acc: 0.45 - ETA: 25s - loss: 2.0723 - acc: 0.50 - ETA: 22s - loss: 1.9930 - acc: 0.51 - ETA: 20s - loss: 1.9086 - acc: 0.53 - ETA: 18s - loss: 1.8461 - acc: 0.53 - ETA: 17s - loss: 1.7803 - acc: 0.53 - ETA: 16s - loss: 1.6884 - acc: 0.55 - ETA: 15s - loss: 1.6301 - acc: 0.56 - ETA: 15s - loss: 1.5783 - acc: 0.56 - ETA: 14s - loss: 1.5220 - acc: 0.57 - ETA: 13s - loss: 1.4917 - acc: 0.57 - ETA: 13s - loss: 1.4599 - acc: 0.57 - ETA: 12s - loss: 1.4359 - acc: 0.57 - ETA: 12s - loss: 1.3981 - acc: 0.57 - ETA: 11s - loss: 1.3834 - acc: 0.57 - ETA: 11s - loss: 1.3633 - acc: 0.56 - ETA: 10s - loss: 1.3410 - acc: 0.55 - ETA: 10s - loss: 1.3247 - acc: 0.55 - ETA: 10s - loss: 1.3025 - acc: 0.55 - ETA: 9s - loss: 1.2825 - acc: 0.5611 - ETA: 9s - loss: 1.2664 - acc: 0.562 - ETA: 9s - loss: 1.2498 - acc: 0.565 - ETA: 8s - loss: 1.2348 - acc: 0.568 - ETA: 8s - loss: 1.2273 - acc: 0.568 - ETA: 8s - loss: 1.2199 - acc: 0.567 - ETA: 7s - loss: 1.2099 - acc: 0.570 - ETA: 7s - loss: 1.1983 - acc: 0.573 - ETA: 7s - loss: 1.1850 - acc: 0.577 - ETA: 7s - loss: 1.1747 - acc: 0.578 - ETA: 6s - loss: 1.1682 - acc: 0.579 - ETA: 6s - loss: 1.1615 - acc: 0.578 - ETA: 6s - loss: 1.1569 - acc: 0.576 - ETA: 5s - loss: 1.1490 - acc: 0.577 - ETA: 5s - loss: 1.1404 - acc: 0.581 - ETA: 5s - loss: 1.1324 - acc: 0.582 - ETA: 5s - loss: 1.1264 - acc: 0.583 - ETA: 4s - loss: 1.1184 - acc: 0.586 - ETA: 4s - loss: 1.1115 - acc: 0.589 - ETA: 4s - loss: 1.1069 - acc: 0.589 - ETA: 3s - loss: 1.0996 - acc: 0.592 - ETA: 3s - loss: 1.0949 - acc: 0.593 - ETA: 3s - loss: 1.0914 - acc: 0.594 - ETA: 3s - loss: 1.0892 - acc: 0.594 - ETA: 2s - loss: 1.0831 - acc: 0.596 - ETA: 2s - loss: 1.0792 - acc: 0.596 - ETA: 2s - loss: 1.0770 - acc: 0.595 - ETA: 2s - loss: 1.0747 - acc: 0.596 - ETA: 1s - loss: 1.0702 - acc: 0.597 - ETA: 1s - loss: 1.0674 - acc: 0.597 - ETA: 1s - loss: 1.0644 - acc: 0.597 - ETA: 1s - loss: 1.0611 - acc: 0.599 - ETA: 0s - loss: 1.0579 - acc: 0.599 - ETA: 0s - loss: 1.0531 - acc: 0.601 - ETA: 0s - loss: 1.0486 - acc: 0.603 - ETA: 0s - loss: 1.0467 - acc: 0.603 - 15s 4ms/step - loss: 1.0452 - acc: 0.6033 - val_loss: 0.8529 - val_acc: 0.6379\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63787, saving model to CNN_best_weights.01-0.6379.hdf5\n",
      "Epoch 2/5\n",
      "3675/3675 [==============================] - ETA: 12s - loss: 0.9830 - acc: 0.54 - ETA: 12s - loss: 0.9449 - acc: 0.57 - ETA: 12s - loss: 0.9561 - acc: 0.55 - ETA: 12s - loss: 0.9124 - acc: 0.58 - ETA: 12s - loss: 0.9181 - acc: 0.59 - ETA: 11s - loss: 0.9138 - acc: 0.59 - ETA: 11s - loss: 0.8967 - acc: 0.60 - ETA: 11s - loss: 0.8927 - acc: 0.60 - ETA: 11s - loss: 0.8756 - acc: 0.62 - ETA: 10s - loss: 0.8780 - acc: 0.62 - ETA: 10s - loss: 0.8781 - acc: 0.61 - ETA: 10s - loss: 0.8814 - acc: 0.61 - ETA: 10s - loss: 0.8791 - acc: 0.61 - ETA: 9s - loss: 0.8712 - acc: 0.6183 - ETA: 9s - loss: 0.8649 - acc: 0.622 - ETA: 9s - loss: 0.8643 - acc: 0.619 - ETA: 9s - loss: 0.8579 - acc: 0.623 - ETA: 9s - loss: 0.8583 - acc: 0.622 - ETA: 8s - loss: 0.8544 - acc: 0.625 - ETA: 8s - loss: 0.8511 - acc: 0.626 - ETA: 8s - loss: 0.8484 - acc: 0.626 - ETA: 8s - loss: 0.8511 - acc: 0.624 - ETA: 7s - loss: 0.8471 - acc: 0.626 - ETA: 7s - loss: 0.8460 - acc: 0.626 - ETA: 7s - loss: 0.8463 - acc: 0.628 - ETA: 7s - loss: 0.8525 - acc: 0.623 - ETA: 6s - loss: 0.8492 - acc: 0.627 - ETA: 6s - loss: 0.8464 - acc: 0.630 - ETA: 6s - loss: 0.8428 - acc: 0.633 - ETA: 6s - loss: 0.8416 - acc: 0.635 - ETA: 6s - loss: 0.8380 - acc: 0.638 - ETA: 5s - loss: 0.8359 - acc: 0.639 - ETA: 5s - loss: 0.8330 - acc: 0.641 - ETA: 5s - loss: 0.8309 - acc: 0.641 - ETA: 5s - loss: 0.8269 - acc: 0.643 - ETA: 4s - loss: 0.8252 - acc: 0.643 - ETA: 4s - loss: 0.8230 - acc: 0.644 - ETA: 4s - loss: 0.8198 - acc: 0.646 - ETA: 4s - loss: 0.8200 - acc: 0.646 - ETA: 3s - loss: 0.8184 - acc: 0.647 - ETA: 3s - loss: 0.8147 - acc: 0.649 - ETA: 3s - loss: 0.8116 - acc: 0.652 - ETA: 3s - loss: 0.8092 - acc: 0.653 - ETA: 3s - loss: 0.8082 - acc: 0.655 - ETA: 2s - loss: 0.8051 - acc: 0.656 - ETA: 2s - loss: 0.8020 - acc: 0.659 - ETA: 2s - loss: 0.8005 - acc: 0.660 - ETA: 2s - loss: 0.7998 - acc: 0.660 - ETA: 1s - loss: 0.7986 - acc: 0.660 - ETA: 1s - loss: 0.7970 - acc: 0.660 - ETA: 1s - loss: 0.7965 - acc: 0.661 - ETA: 1s - loss: 0.7920 - acc: 0.664 - ETA: 1s - loss: 0.7882 - acc: 0.667 - ETA: 0s - loss: 0.7853 - acc: 0.668 - ETA: 0s - loss: 0.7837 - acc: 0.669 - ETA: 0s - loss: 0.7808 - acc: 0.671 - ETA: 0s - loss: 0.7766 - acc: 0.673 - 14s 4ms/step - loss: 0.7772 - acc: 0.6732 - val_loss: 0.7227 - val_acc: 0.7179\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63787 to 0.71792, saving model to CNN_best_weights.02-0.7179.hdf5\n",
      "Epoch 3/5\n",
      "3675/3675 [==============================] - ETA: 13s - loss: 0.5386 - acc: 0.76 - ETA: 13s - loss: 0.5308 - acc: 0.76 - ETA: 13s - loss: 0.5363 - acc: 0.77 - ETA: 13s - loss: 0.5177 - acc: 0.79 - ETA: 13s - loss: 0.4970 - acc: 0.80 - ETA: 12s - loss: 0.5123 - acc: 0.79 - ETA: 12s - loss: 0.5201 - acc: 0.79 - ETA: 12s - loss: 0.5158 - acc: 0.79 - ETA: 11s - loss: 0.5080 - acc: 0.80 - ETA: 11s - loss: 0.5256 - acc: 0.78 - ETA: 11s - loss: 0.5126 - acc: 0.79 - ETA: 11s - loss: 0.5053 - acc: 0.80 - ETA: 10s - loss: 0.5044 - acc: 0.80 - ETA: 10s - loss: 0.5036 - acc: 0.80 - ETA: 10s - loss: 0.4957 - acc: 0.80 - ETA: 9s - loss: 0.4937 - acc: 0.8037 - ETA: 9s - loss: 0.4911 - acc: 0.807 - ETA: 9s - loss: 0.4913 - acc: 0.806 - ETA: 9s - loss: 0.4862 - acc: 0.810 - ETA: 8s - loss: 0.4815 - acc: 0.814 - ETA: 8s - loss: 0.4784 - acc: 0.816 - ETA: 8s - loss: 0.4783 - acc: 0.814 - ETA: 8s - loss: 0.4822 - acc: 0.812 - ETA: 7s - loss: 0.4819 - acc: 0.811 - ETA: 7s - loss: 0.4809 - acc: 0.813 - ETA: 7s - loss: 0.4795 - acc: 0.814 - ETA: 7s - loss: 0.4784 - acc: 0.814 - ETA: 6s - loss: 0.4752 - acc: 0.814 - ETA: 6s - loss: 0.4730 - acc: 0.814 - ETA: 6s - loss: 0.4728 - acc: 0.814 - ETA: 6s - loss: 0.4684 - acc: 0.816 - ETA: 5s - loss: 0.4659 - acc: 0.816 - ETA: 5s - loss: 0.4694 - acc: 0.815 - ETA: 5s - loss: 0.4711 - acc: 0.814 - ETA: 5s - loss: 0.4699 - acc: 0.816 - ETA: 4s - loss: 0.4723 - acc: 0.815 - ETA: 4s - loss: 0.4754 - acc: 0.812 - ETA: 4s - loss: 0.4737 - acc: 0.813 - ETA: 4s - loss: 0.4708 - acc: 0.816 - ETA: 4s - loss: 0.4702 - acc: 0.816 - ETA: 3s - loss: 0.4680 - acc: 0.817 - ETA: 3s - loss: 0.4664 - acc: 0.817 - ETA: 3s - loss: 0.4658 - acc: 0.817 - ETA: 3s - loss: 0.4624 - acc: 0.819 - ETA: 2s - loss: 0.4642 - acc: 0.817 - ETA: 2s - loss: 0.4622 - acc: 0.818 - ETA: 2s - loss: 0.4617 - acc: 0.819 - ETA: 2s - loss: 0.4625 - acc: 0.820 - ETA: 1s - loss: 0.4601 - acc: 0.820 - ETA: 1s - loss: 0.4606 - acc: 0.819 - ETA: 1s - loss: 0.4606 - acc: 0.819 - ETA: 1s - loss: 0.4592 - acc: 0.819 - ETA: 1s - loss: 0.4603 - acc: 0.818 - ETA: 0s - loss: 0.4599 - acc: 0.817 - ETA: 0s - loss: 0.4608 - acc: 0.817 - ETA: 0s - loss: 0.4597 - acc: 0.817 - ETA: 0s - loss: 0.4559 - acc: 0.818 - 14s 4ms/step - loss: 0.4558 - acc: 0.8182 - val_loss: 0.8001 - val_acc: 0.6671\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.71792\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675/3675 [==============================] - ETA: 12s - loss: 0.2359 - acc: 0.92 - ETA: 12s - loss: 0.2286 - acc: 0.93 - ETA: 12s - loss: 0.2111 - acc: 0.95 - ETA: 12s - loss: 0.2119 - acc: 0.95 - ETA: 11s - loss: 0.2107 - acc: 0.95 - ETA: 11s - loss: 0.2031 - acc: 0.96 - ETA: 11s - loss: 0.2020 - acc: 0.96 - ETA: 11s - loss: 0.2000 - acc: 0.95 - ETA: 11s - loss: 0.2046 - acc: 0.95 - ETA: 10s - loss: 0.2081 - acc: 0.95 - ETA: 10s - loss: 0.2048 - acc: 0.95 - ETA: 10s - loss: 0.2022 - acc: 0.95 - ETA: 10s - loss: 0.1976 - acc: 0.95 - ETA: 10s - loss: 0.1928 - acc: 0.95 - ETA: 9s - loss: 0.1886 - acc: 0.9542 - ETA: 9s - loss: 0.1883 - acc: 0.954 - ETA: 9s - loss: 0.1883 - acc: 0.951 - ETA: 9s - loss: 0.1851 - acc: 0.952 - ETA: 8s - loss: 0.1853 - acc: 0.952 - ETA: 8s - loss: 0.1824 - acc: 0.953 - ETA: 8s - loss: 0.1792 - acc: 0.954 - ETA: 8s - loss: 0.1805 - acc: 0.953 - ETA: 8s - loss: 0.1822 - acc: 0.952 - ETA: 7s - loss: 0.1842 - acc: 0.951 - ETA: 7s - loss: 0.1820 - acc: 0.951 - ETA: 7s - loss: 0.1788 - acc: 0.953 - ETA: 7s - loss: 0.1762 - acc: 0.954 - ETA: 6s - loss: 0.1737 - acc: 0.956 - ETA: 6s - loss: 0.1724 - acc: 0.956 - ETA: 6s - loss: 0.1773 - acc: 0.955 - ETA: 6s - loss: 0.1811 - acc: 0.954 - ETA: 5s - loss: 0.1807 - acc: 0.954 - ETA: 5s - loss: 0.1812 - acc: 0.954 - ETA: 5s - loss: 0.1788 - acc: 0.955 - ETA: 5s - loss: 0.1781 - acc: 0.954 - ETA: 5s - loss: 0.1780 - acc: 0.955 - ETA: 4s - loss: 0.1755 - acc: 0.956 - ETA: 4s - loss: 0.1740 - acc: 0.957 - ETA: 4s - loss: 0.1737 - acc: 0.957 - ETA: 4s - loss: 0.1716 - acc: 0.957 - ETA: 3s - loss: 0.1725 - acc: 0.957 - ETA: 3s - loss: 0.1713 - acc: 0.958 - ETA: 3s - loss: 0.1718 - acc: 0.957 - ETA: 3s - loss: 0.1702 - acc: 0.957 - ETA: 2s - loss: 0.1692 - acc: 0.958 - ETA: 2s - loss: 0.1689 - acc: 0.957 - ETA: 2s - loss: 0.1683 - acc: 0.958 - ETA: 2s - loss: 0.1672 - acc: 0.958 - ETA: 1s - loss: 0.1666 - acc: 0.958 - ETA: 1s - loss: 0.1667 - acc: 0.958 - ETA: 1s - loss: 0.1684 - acc: 0.958 - ETA: 1s - loss: 0.1698 - acc: 0.957 - ETA: 1s - loss: 0.1685 - acc: 0.957 - ETA: 0s - loss: 0.1678 - acc: 0.957 - ETA: 0s - loss: 0.1688 - acc: 0.956 - ETA: 0s - loss: 0.1718 - acc: 0.955 - ETA: 0s - loss: 0.1709 - acc: 0.955 - 14s 4ms/step - loss: 0.1700 - acc: 0.9556 - val_loss: 1.0005 - val_acc: 0.6849\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.71792\n",
      "Epoch 5/5\n",
      "3675/3675 [==============================] - ETA: 12s - loss: 0.1258 - acc: 0.93 - ETA: 12s - loss: 0.0941 - acc: 0.96 - ETA: 12s - loss: 0.0758 - acc: 0.96 - ETA: 12s - loss: 0.0718 - acc: 0.97 - ETA: 12s - loss: 0.0803 - acc: 0.97 - ETA: 11s - loss: 0.0798 - acc: 0.97 - ETA: 11s - loss: 0.0764 - acc: 0.97 - ETA: 11s - loss: 0.0715 - acc: 0.97 - ETA: 11s - loss: 0.0674 - acc: 0.97 - ETA: 11s - loss: 0.0687 - acc: 0.97 - ETA: 10s - loss: 0.0736 - acc: 0.97 - ETA: 10s - loss: 0.0694 - acc: 0.98 - ETA: 10s - loss: 0.0679 - acc: 0.98 - ETA: 10s - loss: 0.0709 - acc: 0.98 - ETA: 9s - loss: 0.0692 - acc: 0.9823 - ETA: 9s - loss: 0.0681 - acc: 0.983 - ETA: 9s - loss: 0.0661 - acc: 0.983 - ETA: 9s - loss: 0.0637 - acc: 0.984 - ETA: 8s - loss: 0.0625 - acc: 0.985 - ETA: 8s - loss: 0.0630 - acc: 0.985 - ETA: 8s - loss: 0.0631 - acc: 0.983 - ETA: 8s - loss: 0.0643 - acc: 0.982 - ETA: 7s - loss: 0.0634 - acc: 0.982 - ETA: 7s - loss: 0.0671 - acc: 0.981 - ETA: 7s - loss: 0.0677 - acc: 0.981 - ETA: 7s - loss: 0.0661 - acc: 0.982 - ETA: 7s - loss: 0.0650 - acc: 0.982 - ETA: 6s - loss: 0.0635 - acc: 0.983 - ETA: 6s - loss: 0.0640 - acc: 0.982 - ETA: 6s - loss: 0.0630 - acc: 0.983 - ETA: 6s - loss: 0.0623 - acc: 0.983 - ETA: 5s - loss: 0.0623 - acc: 0.983 - ETA: 5s - loss: 0.0636 - acc: 0.983 - ETA: 5s - loss: 0.0631 - acc: 0.983 - ETA: 5s - loss: 0.0631 - acc: 0.983 - ETA: 4s - loss: 0.0627 - acc: 0.983 - ETA: 4s - loss: 0.0643 - acc: 0.983 - ETA: 4s - loss: 0.0637 - acc: 0.983 - ETA: 4s - loss: 0.0656 - acc: 0.982 - ETA: 4s - loss: 0.0654 - acc: 0.982 - ETA: 3s - loss: 0.0660 - acc: 0.982 - ETA: 3s - loss: 0.0655 - acc: 0.982 - ETA: 3s - loss: 0.0643 - acc: 0.982 - ETA: 3s - loss: 0.0646 - acc: 0.983 - ETA: 2s - loss: 0.0656 - acc: 0.982 - ETA: 2s - loss: 0.0663 - acc: 0.982 - ETA: 2s - loss: 0.0671 - acc: 0.982 - ETA: 2s - loss: 0.0667 - acc: 0.983 - ETA: 1s - loss: 0.0659 - acc: 0.983 - ETA: 1s - loss: 0.0653 - acc: 0.983 - ETA: 1s - loss: 0.0646 - acc: 0.984 - ETA: 1s - loss: 0.0647 - acc: 0.983 - ETA: 1s - loss: 0.0657 - acc: 0.983 - ETA: 0s - loss: 0.0663 - acc: 0.983 - ETA: 0s - loss: 0.0670 - acc: 0.983 - ETA: 0s - loss: 0.0675 - acc: 0.983 - ETA: 0s - loss: 0.0668 - acc: 0.983 - 14s 4ms/step - loss: 0.0668 - acc: 0.9831 - val_loss: 1.0592 - val_acc: 0.6684\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.71792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2651c412c50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, y_train, batch_size=64, epochs=5,\n",
    "                     validation_data=(x_val_seq, y_validation), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#tvec = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "#tvec.fit(x_train)\n",
    "\n",
    "#x_train_tfidf = tvec.transform(x_train)\n",
    "#x_test_tfidf = tvec.transform(x_test)\n",
    "#lr_with_tfidf = LogisticRegression()\n",
    "#lr_with_tfidf.fit(x_train_tfidf,y_train)\n",
    "#lr_with_tfidf.score(x_test_tfidf,y_test)\n",
    "#yhat_lr = lr_with_tfidf.predict_proba(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_CNN_model = load_model('CNN_best_weights.02-0.7179.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_seq = pad_sequences(sequences_test, maxlen=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788/788 [==============================] - ETA:  - ETA:  - ETA:  - 0s 318us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7664900667171188, 0.6928934010152284]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_CNN_model.evaluate(x=x_test_seq, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_cnn = loaded_CNN_model.predict(x_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DataSet Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Reading Test Dataset\n",
    "def read_data(tweets_data_path):\n",
    "    tweets_data = []\n",
    "    tweets_file = open(tweets_data_path, \"r\")\n",
    "    line = tweets_file.readline()\n",
    "    for line in tweets_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    data = pd.DataFrame.from_dict(tweets_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Test Dataset\n",
    "tweet_data=read_data(\"D:/Thesis/autonomousDriveMulti.json\")\n",
    "tweet_data1=read_data(\"D:/Thesis/autonomousDrive.json\")\n",
    "tweet_data2=read_data(\"D:/Thesis/python.json\")\n",
    "tweets= pd.concat([tweet_data, tweet_data1,tweet_data2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test Dataset Only\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Languages', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 languages', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:5].plot(ax=ax, kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test Dataset Only\n",
    "tweets_by_lang = tweets['source'].value_counts()\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Source', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 languages', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:5].plot(ax=ax, kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_data_path = 'D:/Thesis/autonomousDrive.json'\n",
    "\n",
    "#tweets_data = []\n",
    "#tweets_file = open(tweets_data_path, \"r\")\n",
    "#line = tweets_file.readline()\n",
    "#for line in tweets_file:\n",
    "#    try:\n",
    "#        tweet = json.loads(line)\n",
    "#        tweets_data.append(tweet)\n",
    "#    except:\n",
    "#        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = pd.DataFrame.from_dict(tweets_data)\n",
    "#tweets = pd.read_excel(\"D:\\\\Thesis\\\\Code\\\\tweet.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion to xlsx (optional)\n",
    "writer = ExcelWriter('tweet.xlsx')\n",
    "tweets.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test Dataset Only\n",
    "tweets['created_at'] = pd.to_datetime(tweets.created_at)\n",
    "tweets['source'] = tweets['source'].str.extract('>(.+?)<', expand=False).str.strip()\n",
    "tweets['pre_clean_len'] = [len(t) for t in tweets.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test Dataset Only\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(tweets.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data Clean\n",
    "x = len(tweets.index)\n",
    "nums = [0,x]\n",
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    if( (i+1)%881 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                    \n",
    "    clean_tweet_texts.append(tweet_cleaner_updated(tweets['text'][i]))\n",
    "    clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(message for message in tweets.text)\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(width = 2400, height = 1600).generate(text)\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='gaussian')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_len=pd.Series(data=tweets['pre_clean_len'].values, index=tweets['created_at'])\n",
    "time_len.plot(figsize=(16, 4), color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df.groupby(level=0).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
