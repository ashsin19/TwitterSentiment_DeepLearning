{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import hmean\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, concatenate, Activation\n",
    "from keras.layers import Flatten, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Train Dataset\n",
    "tweets=pd.read_csv(\"D:/Thesis/Code/Data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>If Google maps can't keep up with road constru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Autonomous cars seem way overhyped given the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Just saw Google self-driving car on I-34. It w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  Two places I'd invest all my money if I could:...\n",
       "1          1  Awesome! Google driverless cars will help the ...\n",
       "2          0  If Google maps can't keep up with road constru...\n",
       "3          0  Autonomous cars seem way overhyped given the t...\n",
       "4          2  Just saw Google self-driving car on I-34. It w..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "tok = WordPunctTokenizer()\n",
    "#stop_words = set(stopwords.words('english')) \n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "pat3 = r'RT @[A-Za-z0-9_]+'\n",
    "pat4 = r'RT @[A-Za-z0-9_ ] RT+'\n",
    "#combined_pat = r'|'.join((pat1, pat2,pat3))\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "rt_pattern= re.compile(pat3,flags=re.IGNORECASE)\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    if rt_pattern.match(bom_removed):\n",
    "        rt_stripped=re.sub(pat3,'',bom_removed)\n",
    "        stripped = re.sub(combined_pat, '', rt_stripped)\n",
    "    else:\n",
    "        stripped = re.sub(combined_pat, '', bom_removed)\n",
    "        stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    #tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Tweets 525 of 5250 has been processed\n",
      "Tweets 1050 of 5250 has been processed\n",
      "Tweets 1575 of 5250 has been processed\n",
      "Tweets 2100 of 5250 has been processed\n",
      "Tweets 2625 of 5250 has been processed\n",
      "Tweets 3150 of 5250 has been processed\n",
      "Tweets 3675 of 5250 has been processed\n",
      "Tweets 4200 of 5250 has been processed\n",
      "Tweets 4725 of 5250 has been processed\n",
      "Tweets 5250 of 5250 has been processed\n"
     ]
    }
   ],
   "source": [
    "x = len(tweets.index)\n",
    "nums = [0,x]\n",
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    if( (i+1)%525 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                    \n",
    "    clean_tweet_texts.append(tweet_cleaner_updated(tweets['text'][i]))\n",
    "    clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['target'] = tweets.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clean_df.text\n",
    "y = clean_df.target\n",
    "SEED = 5\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5128108.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2629579.17it/s]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 15:36:39.027686 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5261671.68it/s]\n",
      "W0813 15:36:39.190251 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 15:36:39.354812 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 15:36:39.523360 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5347279.26it/s]\n",
      "W0813 15:36:39.688477 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2630521.56it/s]\n",
      "W0813 15:36:39.863010 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5264187.43it/s]\n",
      "W0813 15:36:40.030534 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5265446.20it/s]\n",
      "W0813 15:36:40.192102 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1300424.97it/s]\n",
      "W0813 15:36:40.364640 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5262929.25it/s]\n",
      "W0813 15:36:40.540171 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:36:40.702736 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5264187.43it/s]\n",
      "W0813 15:36:40.879264 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5264187.43it/s]\n",
      "W0813 15:36:41.049322 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 15:36:41.215418 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5257902.58it/s]\n",
      "W0813 15:36:41.391500 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633982.78it/s]\n",
      "W0813 15:36:41.553721 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2599161.47it/s]\n",
      "W0813 15:36:41.719277 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633667.74it/s]\n",
      "W0813 15:36:41.883837 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5267965.55it/s]\n",
      "W0813 15:36:42.051390 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5261671.68it/s]\n",
      "W0813 15:36:42.214952 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 15:36:42.387493 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:36:42.559032 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5270487.31it/s]\n",
      "W0813 15:36:42.723883 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 15:36:42.895425 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5262929.25it/s]\n",
      "W0813 15:36:43.067215 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5264187.43it/s]\n",
      "W0813 15:36:43.232771 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633667.74it/s]\n",
      "W0813 15:36:43.399326 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1755008.85it/s]\n",
      "W0813 15:36:43.568872 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5260414.72it/s]\n",
      "W0813 15:36:43.734429 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5265446.20it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 15:36:43.906968 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6696315120711563"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pure Distributed Bag of Words\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dbow.alpha -= 0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha\n",
    "    \n",
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "    return vecs\n",
    "  \n",
    "train_vecs_dbow = get_vectors(model_ug_dbow, x_train, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, x_validation, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow, y_train)\n",
    "clf.score(validation_vecs_dbow, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2592124.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754868.98it/s]\n",
      "W0813 15:36:46.176894 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632093.71it/s]\n",
      "W0813 15:36:46.418249 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5264187.43it/s]\n",
      "W0813 15:36:46.692630 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631150.20it/s]\n",
      "W0813 15:36:46.976869 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5269226.13it/s]\n",
      "W0813 15:36:47.263104 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5269226.13it/s]\n",
      "W0813 15:36:47.550335 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754309.75it/s]\n",
      "W0813 15:36:47.799669 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754589.32it/s]\n",
      "W0813 15:36:48.064960 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5260414.72it/s]\n",
      "W0813 15:36:48.363162 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1755148.73it/s]\n",
      "W0813 15:36:48.648399 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1753890.56it/s]\n",
      "W0813 15:36:48.920734 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 15:36:49.195998 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754868.98it/s]\n",
      "W0813 15:36:49.524121 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5308605.59it/s]\n",
      "W0813 15:36:49.760489 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5260414.72it/s]\n",
      "W0813 15:36:50.011817 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2705171.50it/s]\n",
      "W0813 15:36:50.214711 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5260414.72it/s]\n",
      "W0813 15:36:50.499554 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2653663.05it/s]\n",
      "W0813 15:36:50.796265 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5269226.13it/s]\n",
      "W0813 15:36:51.079507 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5264187.43it/s]\n",
      "W0813 15:36:51.363747 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5261671.68it/s]\n",
      "W0813 15:36:51.645517 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5269226.13it/s]\n",
      "W0813 15:36:51.925307 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 15:36:52.253429 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2630835.84it/s]\n",
      "W0813 15:36:52.463866 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2630207.36it/s]\n",
      "W0813 15:36:52.705220 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2629893.23it/s]\n",
      "W0813 15:36:52.957546 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:36:53.183528 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5267965.55it/s]\n",
      "W0813 15:36:53.470330 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5267965.55it/s]\n",
      "W0813 15:36:53.736723 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 15:36:54.015976 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6531130876747141"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pure Distributed Memory Mean\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmm = Doc2Vec(dm=1, dm_mean=1, size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmm.alpha -= 0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha\n",
    "    \n",
    "train_vecs_dmm = get_vectors(model_ug_dmm, x_train, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, x_validation, 100)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, y_train)\n",
    "clf.score(validation_vecs_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6670902160101652"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DBOW+DMM\n",
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_train, 200)\n",
    "validation_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_validation, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm, y_train)\n",
    "clf.score(validation_vecs_dbow_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2634297.88it/s]\n"
     ]
    }
   ],
   "source": [
    "#Separate Word2Vec\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2,\n",
    "                         workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632093.71it/s]\n",
      "W0813 15:37:01.637448 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2647600.82it/s]\n",
      "W0813 15:37:01.686319 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1754170.00it/s]\n",
      "W0813 15:37:01.729203 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5091351.68it/s]\n",
      "W0813 15:37:01.772123 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:37:01.814014 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5076094.05it/s]\n",
      "W0813 15:37:01.854867 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:37:01.897783 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 15:37:01.940640 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:37:01.980566 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5269226.13it/s]\n",
      "W0813 15:37:02.024445 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 15:37:02.064338 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:37:02.107192 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:37:02.149111 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2586339.68it/s]\n",
      "W0813 15:37:02.195988 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5109071.00it/s]\n",
      "W0813 15:37:02.237875 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2588467.85it/s]\n",
      "W0813 15:37:02.278767 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2590294.79it/s]\n",
      "W0813 15:37:02.321654 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 3159721.05it/s]\n",
      "W0813 15:37:02.364133 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633352.79it/s]\n",
      "W0813 15:37:02.405056 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2590294.79it/s]\n",
      "W0813 15:37:02.447947 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 15:37:02.488804 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632093.71it/s]\n",
      "W0813 15:37:02.531232 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:37:02.574118 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5259158.35it/s]\n",
      "W0813 15:37:02.616005 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2587251.32it/s]\n",
      "W0813 15:37:02.659859 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2639349.87it/s]\n",
      "W0813 15:37:02.708725 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 1755148.73it/s]\n",
      "W0813 15:37:02.754635 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 15:37:02.794495 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:37:02.839376 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 15:37:02.885253 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5264187.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 15:37:03.000974 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2605928.52it/s]\n",
      "W0813 15:37:03.049813 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2577258.43it/s]\n",
      "W0813 15:37:03.101674 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5266705.57it/s]\n",
      "W0813 15:37:03.150576 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2589685.52it/s]\n",
      "W0813 15:37:03.202436 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 15:37:03.252272 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:37:03.304133 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5113817.00it/s]\n",
      "W0813 15:37:03.354996 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 15:37:03.405861 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:37:03.457721 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:37:03.506591 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632093.71it/s]\n",
      "W0813 15:37:03.558452 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:37:03.607321 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 5265446.20it/s]\n",
      "W0813 15:37:03.655200 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 15:37:03.703073 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:37:03.758923 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 15:37:03.809788 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633667.74it/s]\n",
      "W0813 15:37:03.858244 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633352.79it/s]\n",
      "W0813 15:37:03.906117 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:37:03.957359 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:37:04.006228 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632408.37it/s]\n",
      "W0813 15:37:04.055606 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:37:04.105472 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2632723.10it/s]\n",
      "W0813 15:37:04.153851 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:37:04.203718 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631464.63it/s]\n",
      "W0813 15:37:04.253584 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 15:37:04.302454 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 15:37:04.352321 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2633037.91it/s]\n",
      "W0813 15:37:04.406177 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5250/5250 [00:00<00:00, 2631779.13it/s]\n",
      "W0813 15:37:04.457041 16860 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6721728081321474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6645489199491741\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]),\n",
    "                        total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_cbow.alpha -= 0.002\n",
    "    model_ug_cbow.min_alpha = model_ug_cbow.alpha\n",
    "\n",
    "model_ug_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2,\n",
    "                       workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]),\n",
    "                      total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_sg.alpha -= 0.002\n",
    "    model_ug_sg.min_alpha = model_ug_sg.alpha\n",
    "\n",
    "def get_w2v_mean(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_cbow[word],model_ug_sg[word]).reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "  \n",
    "def get_w2v_sum(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_cbow[word],model_ug_sg[word]).reshape((1, size))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return vec\n",
    "  \n",
    "train_vecs_cbowsg_mean = scale(np.concatenate([get_w2v_mean(z, 200) for z in x_train]))\n",
    "validation_vecs_cbowsg_mean = scale(np.concatenate([get_w2v_mean(z, 200) for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_cbowsg_mean, y_train)\n",
    "print(clf.score(validation_vecs_cbowsg_mean, y_validation))\n",
    "\n",
    "train_vecs_cbowsg_sum = scale(np.concatenate([get_w2v_sum(z, 200) for z in x_train]))\n",
    "validation_vecs_cbowsg_sum = scale(np.concatenate([get_w2v_sum(z, 200) for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_cbowsg_sum, y_train)\n",
    "print(clf.score(validation_vecs_cbowsg_sum, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v general\n",
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6747141041931385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6670902160101652\n"
     ]
    }
   ],
   "source": [
    "#Word vectors extracted from Doc2Vec models with custom weighting\n",
    "cvec = CountVectorizer(max_features=100000)\n",
    "cvec.fit(x_train)\n",
    "\n",
    "neg_train = x_train[y_train == 0]\n",
    "pos_train = x_train[y_train == 1]\n",
    "neu_train = x_train[y_train == 2]\n",
    "neg_doc_matrix = cvec.transform(neg_train)\n",
    "pos_doc_matrix = cvec.transform(pos_train)\n",
    "neu_doc_matrix = cvec.transform(neu_train)\n",
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
    "neu_tf = np.sum(neu_doc_matrix,axis=0)\n",
    "\n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "neu = np.squeeze(np.asarray(neu_tf))\n",
    "term_freq_df2 = pd.DataFrame([neg,pos,neu],columns=cvec.get_feature_names()).transpose()\n",
    "term_freq_df2.columns = ['negative', 'positive','neutral']\n",
    "term_freq_df2['total'] = term_freq_df2['negative'] + term_freq_df2['positive'] + term_freq_df2['neutral']\n",
    "term_freq_df2['pos_rate'] = term_freq_df2['positive'] * 1./term_freq_df2['total']\n",
    "term_freq_df2['pos_freq_pct'] = term_freq_df2['positive'] * 1./term_freq_df2['positive'].sum()\n",
    "term_freq_df2['pos_rate_normcdf'] = normcdf(term_freq_df2['pos_rate'])\n",
    "term_freq_df2['pos_freq_pct_normcdf'] = normcdf(term_freq_df2['pos_freq_pct'])\n",
    "term_freq_df2['pos_normcdf_hmean'] = hmean([term_freq_df2['pos_rate_normcdf'], term_freq_df2['pos_freq_pct_normcdf']])\n",
    "pos_hmean = term_freq_df2.pos_normcdf_hmean\n",
    "\n",
    "w2v_pos_hmean = {}\n",
    "for w in model_ug_dbow.wv.vocab.keys():\n",
    "    if w in pos_hmean.keys():\n",
    "        w2v_pos_hmean[w] = np.append(model_ug_dbow[w],model_ug_dmm[w]) * pos_hmean[w]\n",
    "        \n",
    "train_vecs_w2v_poshmean_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean, 'mean') for z in x_train]))\n",
    "validation_vecs_w2v_poshmean_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean, 'mean') for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_mean, y_train)\n",
    "print(clf.score(validation_vecs_w2v_poshmean_mean, y_validation))\n",
    "\n",
    "train_vecs_w2v_poshmean_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean, 'sum') for z in x_train]))\n",
    "validation_vecs_w2v_poshmean_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean, 'sum') for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_sum, y_train)\n",
    "print(clf.score(validation_vecs_w2v_poshmean_sum, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6658195679796697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6772554002541296\n"
     ]
    }
   ],
   "source": [
    "#Separately trained Word2Vec with custom weighting (Average/Sum)\n",
    "w2v_pos_hmean_01 = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    if w in pos_hmean.keys():\n",
    "        w2v_pos_hmean_01[w] = np.append(model_ug_cbow[w],model_ug_sg[w]) * pos_hmean[w]\n",
    "        \n",
    "train_vecs_w2v_poshmean_mean_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'mean') for z in x_train]))\n",
    "validation_vecs_w2v_poshmean_mean_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'mean') for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_mean_01, y_train)\n",
    "print(clf.score(validation_vecs_w2v_poshmean_mean_01, y_validation))\n",
    "\n",
    "train_vecs_w2v_poshmean_sum_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'sum') for z in x_train]))\n",
    "validation_vecs_w2v_poshmean_sum_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_pos_hmean_01, 'sum') for z in x_validation]))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_poshmean_sum_01, y_train)\n",
    "print(clf.score(validation_vecs_w2v_poshmean_sum_01, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow.save(\"w2v_model_ug_cbow.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_sg.save(\"w2v_model_ug_sg.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_ug_cbow = KeyedVectors.load('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=45)\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 15:37:42.636605 16860 deprecation_wrapper.py:119] From C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0813 15:37:42.651576 16860 deprecation_wrapper.py:119] From C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0813 15:37:42.655541 16860 deprecation_wrapper.py:119] From C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0813 15:37:42.687489 16860 deprecation_wrapper.py:119] From C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0813 15:37:42.702416 16860 deprecation_wrapper.py:119] From C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0813 15:37:42.764250 16860 deprecation.py:323] From C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0813 15:37:42.793185 16860 deprecation_wrapper.py:119] From C:\\Users\\Asus FX504\\.conda\\envs\\tensor\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3675 samples, validate on 787 samples\n",
      "Epoch 1/5\n",
      " - 15s - loss: 0.9307 - acc: 0.6180 - val_loss: 0.8163 - val_acc: 0.6480\n",
      "Epoch 2/5\n",
      " - 14s - loss: 0.5617 - acc: 0.7875 - val_loss: 0.7838 - val_acc: 0.6595\n",
      "Epoch 3/5\n",
      " - 15s - loss: 0.1850 - acc: 0.9488 - val_loss: 0.9097 - val_acc: 0.6760\n",
      "Epoch 4/5\n",
      " - 15s - loss: 0.0664 - acc: 0.9837 - val_loss: 0.9723 - val_acc: 0.6633\n",
      "Epoch 5/5\n",
      " - 16s - loss: 0.0436 - acc: 0.9878 - val_loss: 1.0342 - val_acc: 0.6607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18317b3fb70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ptw2v = Sequential()\n",
    "e = Embedding(100000, 200, input_length=45)\n",
    "model_ptw2v.add(e)\n",
    "model_ptw2v.add(Flatten())\n",
    "model_ptw2v.add(Dense(512, activation='relu'))\n",
    "model_ptw2v.add(Dense(10, activation='softmax'))\n",
    "model_ptw2v.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 45, 200)      20000000    input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 44, 100)      40100       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 43, 100)      60100       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 42, 100)      80100       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 100)          0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 100)          0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 100)          0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 300)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "                                                                 global_max_pooling1d_24[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 512)          154112      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 512)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 10)           5130        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 10)           0           dense_18[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 20,339,542\n",
      "Trainable params: 20,339,542\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tweet_input = Input(shape=(45,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(100000, 200, input_length=45)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(512, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(10)(merged)\n",
    "output = Activation('softmax')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3675 samples, validate on 787 samples\n",
      "Epoch 1/10\n",
      "3675/3675 [==============================] - ETA: 1:32 - loss: 2.3236 - acc: 0.0000e+0 - ETA: 52s - loss: 2.2378 - acc: 0.2656    - ETA: 38s - loss: 2.1641 - acc: 0.36 - ETA: 31s - loss: 2.1004 - acc: 0.41 - ETA: 27s - loss: 2.0398 - acc: 0.43 - ETA: 25s - loss: 1.9614 - acc: 0.46 - ETA: 23s - loss: 1.8842 - acc: 0.48 - ETA: 21s - loss: 1.8192 - acc: 0.49 - ETA: 20s - loss: 1.7378 - acc: 0.51 - ETA: 18s - loss: 1.6684 - acc: 0.52 - ETA: 17s - loss: 1.5987 - acc: 0.54 - ETA: 16s - loss: 1.5719 - acc: 0.54 - ETA: 15s - loss: 1.5209 - acc: 0.55 - ETA: 15s - loss: 1.4878 - acc: 0.55 - ETA: 14s - loss: 1.4625 - acc: 0.55 - ETA: 13s - loss: 1.4200 - acc: 0.56 - ETA: 13s - loss: 1.3949 - acc: 0.56 - ETA: 12s - loss: 1.3641 - acc: 0.57 - ETA: 12s - loss: 1.3349 - acc: 0.58 - ETA: 11s - loss: 1.3127 - acc: 0.58 - ETA: 11s - loss: 1.2998 - acc: 0.58 - ETA: 10s - loss: 1.2750 - acc: 0.58 - ETA: 10s - loss: 1.2568 - acc: 0.58 - ETA: 10s - loss: 1.2397 - acc: 0.59 - ETA: 9s - loss: 1.2284 - acc: 0.5913 - ETA: 9s - loss: 1.2192 - acc: 0.590 - ETA: 8s - loss: 1.2118 - acc: 0.590 - ETA: 8s - loss: 1.1963 - acc: 0.593 - ETA: 8s - loss: 1.1869 - acc: 0.594 - ETA: 7s - loss: 1.1806 - acc: 0.592 - ETA: 7s - loss: 1.1699 - acc: 0.593 - ETA: 7s - loss: 1.1609 - acc: 0.593 - ETA: 6s - loss: 1.1538 - acc: 0.592 - ETA: 6s - loss: 1.1449 - acc: 0.593 - ETA: 6s - loss: 1.1369 - acc: 0.596 - ETA: 5s - loss: 1.1296 - acc: 0.598 - ETA: 5s - loss: 1.1224 - acc: 0.600 - ETA: 5s - loss: 1.1107 - acc: 0.604 - ETA: 5s - loss: 1.1061 - acc: 0.605 - ETA: 4s - loss: 1.1069 - acc: 0.603 - ETA: 4s - loss: 1.1013 - acc: 0.605 - ETA: 4s - loss: 1.0974 - acc: 0.605 - ETA: 3s - loss: 1.0938 - acc: 0.605 - ETA: 3s - loss: 1.0902 - acc: 0.605 - ETA: 3s - loss: 1.0847 - acc: 0.607 - ETA: 3s - loss: 1.0807 - acc: 0.608 - ETA: 2s - loss: 1.0776 - acc: 0.607 - ETA: 2s - loss: 1.0747 - acc: 0.607 - ETA: 2s - loss: 1.0722 - acc: 0.606 - ETA: 1s - loss: 1.0685 - acc: 0.606 - ETA: 1s - loss: 1.0668 - acc: 0.605 - ETA: 1s - loss: 1.0645 - acc: 0.604 - ETA: 1s - loss: 1.0608 - acc: 0.606 - ETA: 0s - loss: 1.0577 - acc: 0.605 - ETA: 0s - loss: 1.0524 - acc: 0.606 - ETA: 0s - loss: 1.0466 - acc: 0.609 - ETA: 0s - loss: 1.0444 - acc: 0.608 - 15s 4ms/step - loss: 1.0445 - acc: 0.6076 - val_loss: 0.8502 - val_acc: 0.6379\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63787, saving model to CNN_best_weights.01-0.64.hdf5\n",
      "Epoch 2/10\n",
      "3675/3675 [==============================] - ETA: 12s - loss: 0.9335 - acc: 0.62 - ETA: 12s - loss: 0.9145 - acc: 0.60 - ETA: 12s - loss: 0.9036 - acc: 0.60 - ETA: 12s - loss: 0.8950 - acc: 0.61 - ETA: 11s - loss: 0.8635 - acc: 0.61 - ETA: 11s - loss: 0.8765 - acc: 0.61 - ETA: 11s - loss: 0.8605 - acc: 0.61 - ETA: 11s - loss: 0.8549 - acc: 0.62 - ETA: 11s - loss: 0.8513 - acc: 0.63 - ETA: 10s - loss: 0.8469 - acc: 0.63 - ETA: 10s - loss: 0.8424 - acc: 0.64 - ETA: 10s - loss: 0.8476 - acc: 0.63 - ETA: 10s - loss: 0.8434 - acc: 0.63 - ETA: 10s - loss: 0.8449 - acc: 0.63 - ETA: 9s - loss: 0.8385 - acc: 0.6417 - ETA: 9s - loss: 0.8371 - acc: 0.641 - ETA: 9s - loss: 0.8347 - acc: 0.640 - ETA: 9s - loss: 0.8327 - acc: 0.640 - ETA: 8s - loss: 0.8302 - acc: 0.641 - ETA: 8s - loss: 0.8268 - acc: 0.641 - ETA: 8s - loss: 0.8269 - acc: 0.638 - ETA: 8s - loss: 0.8305 - acc: 0.637 - ETA: 7s - loss: 0.8273 - acc: 0.641 - ETA: 7s - loss: 0.8249 - acc: 0.643 - ETA: 7s - loss: 0.8283 - acc: 0.641 - ETA: 7s - loss: 0.8329 - acc: 0.637 - ETA: 7s - loss: 0.8322 - acc: 0.638 - ETA: 6s - loss: 0.8322 - acc: 0.639 - ETA: 6s - loss: 0.8305 - acc: 0.639 - ETA: 6s - loss: 0.8266 - acc: 0.641 - ETA: 6s - loss: 0.8238 - acc: 0.644 - ETA: 5s - loss: 0.8249 - acc: 0.643 - ETA: 5s - loss: 0.8209 - acc: 0.644 - ETA: 5s - loss: 0.8186 - acc: 0.647 - ETA: 5s - loss: 0.8206 - acc: 0.646 - ETA: 4s - loss: 0.8158 - acc: 0.648 - ETA: 4s - loss: 0.8153 - acc: 0.649 - ETA: 4s - loss: 0.8103 - acc: 0.650 - ETA: 4s - loss: 0.8116 - acc: 0.649 - ETA: 4s - loss: 0.8062 - acc: 0.653 - ETA: 3s - loss: 0.8043 - acc: 0.654 - ETA: 3s - loss: 0.7992 - acc: 0.657 - ETA: 3s - loss: 0.7966 - acc: 0.659 - ETA: 3s - loss: 0.7966 - acc: 0.659 - ETA: 2s - loss: 0.7960 - acc: 0.659 - ETA: 2s - loss: 0.7925 - acc: 0.660 - ETA: 2s - loss: 0.7904 - acc: 0.661 - ETA: 2s - loss: 0.7913 - acc: 0.662 - ETA: 1s - loss: 0.7902 - acc: 0.663 - ETA: 1s - loss: 0.7896 - acc: 0.663 - ETA: 1s - loss: 0.7849 - acc: 0.667 - ETA: 1s - loss: 0.7844 - acc: 0.667 - ETA: 1s - loss: 0.7822 - acc: 0.667 - ETA: 0s - loss: 0.7787 - acc: 0.669 - ETA: 0s - loss: 0.7784 - acc: 0.669 - ETA: 0s - loss: 0.7778 - acc: 0.668 - ETA: 0s - loss: 0.7741 - acc: 0.670 - 13s 4ms/step - loss: 0.7746 - acc: 0.6705 - val_loss: 0.7317 - val_acc: 0.7052\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63787 to 0.70521, saving model to CNN_best_weights.02-0.71.hdf5\n",
      "Epoch 3/10\n",
      "3675/3675 [==============================] - ETA: 12s - loss: 0.5394 - acc: 0.82 - ETA: 12s - loss: 0.5146 - acc: 0.83 - ETA: 12s - loss: 0.5069 - acc: 0.84 - ETA: 12s - loss: 0.5208 - acc: 0.82 - ETA: 11s - loss: 0.5100 - acc: 0.81 - ETA: 11s - loss: 0.5022 - acc: 0.81 - ETA: 11s - loss: 0.5055 - acc: 0.81 - ETA: 11s - loss: 0.4980 - acc: 0.80 - ETA: 10s - loss: 0.4866 - acc: 0.81 - ETA: 10s - loss: 0.4704 - acc: 0.82 - ETA: 10s - loss: 0.4868 - acc: 0.81 - ETA: 10s - loss: 0.4913 - acc: 0.81 - ETA: 10s - loss: 0.4886 - acc: 0.81 - ETA: 9s - loss: 0.4812 - acc: 0.8225 - ETA: 9s - loss: 0.4810 - acc: 0.821 - ETA: 9s - loss: 0.4856 - acc: 0.821 - ETA: 9s - loss: 0.4851 - acc: 0.820 - ETA: 9s - loss: 0.4878 - acc: 0.817 - ETA: 8s - loss: 0.4872 - acc: 0.819 - ETA: 8s - loss: 0.4872 - acc: 0.817 - ETA: 8s - loss: 0.4840 - acc: 0.819 - ETA: 8s - loss: 0.4818 - acc: 0.820 - ETA: 7s - loss: 0.4878 - acc: 0.815 - ETA: 7s - loss: 0.4856 - acc: 0.817 - ETA: 7s - loss: 0.4832 - acc: 0.818 - ETA: 7s - loss: 0.4867 - acc: 0.819 - ETA: 6s - loss: 0.4806 - acc: 0.822 - ETA: 6s - loss: 0.4789 - acc: 0.822 - ETA: 6s - loss: 0.4731 - acc: 0.823 - ETA: 6s - loss: 0.4747 - acc: 0.822 - ETA: 6s - loss: 0.4801 - acc: 0.820 - ETA: 5s - loss: 0.4773 - acc: 0.820 - ETA: 5s - loss: 0.4814 - acc: 0.816 - ETA: 5s - loss: 0.4820 - acc: 0.815 - ETA: 5s - loss: 0.4785 - acc: 0.816 - ETA: 4s - loss: 0.4796 - acc: 0.813 - ETA: 4s - loss: 0.4805 - acc: 0.813 - ETA: 4s - loss: 0.4839 - acc: 0.812 - ETA: 4s - loss: 0.4850 - acc: 0.812 - ETA: 3s - loss: 0.4844 - acc: 0.813 - ETA: 3s - loss: 0.4859 - acc: 0.813 - ETA: 3s - loss: 0.4857 - acc: 0.814 - ETA: 3s - loss: 0.4848 - acc: 0.816 - ETA: 3s - loss: 0.4843 - acc: 0.816 - ETA: 2s - loss: 0.4813 - acc: 0.817 - ETA: 2s - loss: 0.4793 - acc: 0.817 - ETA: 2s - loss: 0.4772 - acc: 0.818 - ETA: 2s - loss: 0.4766 - acc: 0.818 - ETA: 1s - loss: 0.4777 - acc: 0.817 - ETA: 1s - loss: 0.4783 - acc: 0.817 - ETA: 1s - loss: 0.4784 - acc: 0.817 - ETA: 1s - loss: 0.4772 - acc: 0.817 - ETA: 1s - loss: 0.4771 - acc: 0.817 - ETA: 0s - loss: 0.4736 - acc: 0.819 - ETA: 0s - loss: 0.4723 - acc: 0.819 - ETA: 0s - loss: 0.4715 - acc: 0.819 - ETA: 0s - loss: 0.4741 - acc: 0.818 - 13s 4ms/step - loss: 0.4733 - acc: 0.8196 - val_loss: 0.8228 - val_acc: 0.6557\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.70521\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675/3675 [==============================] - ETA: 12s - loss: 0.2222 - acc: 0.95 - ETA: 12s - loss: 0.2437 - acc: 0.93 - ETA: 12s - loss: 0.2466 - acc: 0.94 - ETA: 12s - loss: 0.2282 - acc: 0.94 - ETA: 11s - loss: 0.2233 - acc: 0.95 - ETA: 11s - loss: 0.2099 - acc: 0.95 - ETA: 11s - loss: 0.2088 - acc: 0.94 - ETA: 11s - loss: 0.2073 - acc: 0.94 - ETA: 11s - loss: 0.2163 - acc: 0.94 - ETA: 10s - loss: 0.2301 - acc: 0.93 - ETA: 10s - loss: 0.2291 - acc: 0.93 - ETA: 10s - loss: 0.2284 - acc: 0.92 - ETA: 10s - loss: 0.2243 - acc: 0.93 - ETA: 9s - loss: 0.2283 - acc: 0.9297 - ETA: 9s - loss: 0.2262 - acc: 0.931 - ETA: 9s - loss: 0.2236 - acc: 0.934 - ETA: 9s - loss: 0.2255 - acc: 0.933 - ETA: 8s - loss: 0.2257 - acc: 0.935 - ETA: 8s - loss: 0.2231 - acc: 0.937 - ETA: 8s - loss: 0.2200 - acc: 0.939 - ETA: 8s - loss: 0.2243 - acc: 0.936 - ETA: 8s - loss: 0.2216 - acc: 0.938 - ETA: 7s - loss: 0.2212 - acc: 0.938 - ETA: 7s - loss: 0.2241 - acc: 0.936 - ETA: 7s - loss: 0.2255 - acc: 0.935 - ETA: 7s - loss: 0.2268 - acc: 0.934 - ETA: 6s - loss: 0.2240 - acc: 0.935 - ETA: 6s - loss: 0.2245 - acc: 0.935 - ETA: 6s - loss: 0.2232 - acc: 0.935 - ETA: 6s - loss: 0.2224 - acc: 0.935 - ETA: 6s - loss: 0.2207 - acc: 0.935 - ETA: 5s - loss: 0.2199 - acc: 0.935 - ETA: 5s - loss: 0.2193 - acc: 0.933 - ETA: 5s - loss: 0.2163 - acc: 0.934 - ETA: 5s - loss: 0.2153 - acc: 0.934 - ETA: 4s - loss: 0.2178 - acc: 0.933 - ETA: 4s - loss: 0.2152 - acc: 0.934 - ETA: 4s - loss: 0.2138 - acc: 0.935 - ETA: 4s - loss: 0.2119 - acc: 0.935 - ETA: 3s - loss: 0.2104 - acc: 0.935 - ETA: 3s - loss: 0.2094 - acc: 0.936 - ETA: 3s - loss: 0.2069 - acc: 0.937 - ETA: 3s - loss: 0.2050 - acc: 0.938 - ETA: 3s - loss: 0.2059 - acc: 0.937 - ETA: 2s - loss: 0.2029 - acc: 0.939 - ETA: 2s - loss: 0.2018 - acc: 0.939 - ETA: 2s - loss: 0.2015 - acc: 0.939 - ETA: 2s - loss: 0.2009 - acc: 0.939 - ETA: 1s - loss: 0.2002 - acc: 0.939 - ETA: 1s - loss: 0.1995 - acc: 0.940 - ETA: 1s - loss: 0.1984 - acc: 0.940 - ETA: 1s - loss: 0.1977 - acc: 0.940 - ETA: 1s - loss: 0.1993 - acc: 0.939 - ETA: 0s - loss: 0.1982 - acc: 0.940 - ETA: 0s - loss: 0.1961 - acc: 0.941 - ETA: 0s - loss: 0.1962 - acc: 0.940 - ETA: 0s - loss: 0.1961 - acc: 0.940 - 14s 4ms/step - loss: 0.1950 - acc: 0.9410 - val_loss: 1.0019 - val_acc: 0.6455\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.70521\n",
      "Epoch 5/10\n",
      "3675/3675 [==============================] - ETA: 12s - loss: 0.0989 - acc: 0.98 - ETA: 12s - loss: 0.1024 - acc: 0.97 - ETA: 12s - loss: 0.0860 - acc: 0.98 - ETA: 12s - loss: 0.0850 - acc: 0.98 - ETA: 12s - loss: 0.0831 - acc: 0.98 - ETA: 12s - loss: 0.0879 - acc: 0.98 - ETA: 11s - loss: 0.0969 - acc: 0.98 - ETA: 11s - loss: 0.1108 - acc: 0.98 - ETA: 11s - loss: 0.1062 - acc: 0.98 - ETA: 11s - loss: 0.1065 - acc: 0.97 - ETA: 11s - loss: 0.1005 - acc: 0.98 - ETA: 10s - loss: 0.1038 - acc: 0.97 - ETA: 10s - loss: 0.1030 - acc: 0.97 - ETA: 10s - loss: 0.1005 - acc: 0.97 - ETA: 10s - loss: 0.0953 - acc: 0.98 - ETA: 9s - loss: 0.0906 - acc: 0.9824 - ETA: 9s - loss: 0.0953 - acc: 0.980 - ETA: 9s - loss: 0.0948 - acc: 0.980 - ETA: 9s - loss: 0.0913 - acc: 0.981 - ETA: 8s - loss: 0.0888 - acc: 0.982 - ETA: 8s - loss: 0.0875 - acc: 0.981 - ETA: 8s - loss: 0.0905 - acc: 0.980 - ETA: 8s - loss: 0.0879 - acc: 0.981 - ETA: 7s - loss: 0.0863 - acc: 0.981 - ETA: 7s - loss: 0.0872 - acc: 0.980 - ETA: 7s - loss: 0.0854 - acc: 0.980 - ETA: 7s - loss: 0.0851 - acc: 0.980 - ETA: 7s - loss: 0.0846 - acc: 0.981 - ETA: 6s - loss: 0.0833 - acc: 0.981 - ETA: 6s - loss: 0.0866 - acc: 0.979 - ETA: 6s - loss: 0.0852 - acc: 0.980 - ETA: 6s - loss: 0.0859 - acc: 0.980 - ETA: 5s - loss: 0.0840 - acc: 0.980 - ETA: 5s - loss: 0.0828 - acc: 0.981 - ETA: 5s - loss: 0.0818 - acc: 0.981 - ETA: 5s - loss: 0.0830 - acc: 0.980 - ETA: 4s - loss: 0.0834 - acc: 0.981 - ETA: 4s - loss: 0.0819 - acc: 0.981 - ETA: 4s - loss: 0.0816 - acc: 0.981 - ETA: 4s - loss: 0.0822 - acc: 0.981 - ETA: 3s - loss: 0.0810 - acc: 0.981 - ETA: 3s - loss: 0.0825 - acc: 0.980 - ETA: 3s - loss: 0.0814 - acc: 0.981 - ETA: 3s - loss: 0.0812 - acc: 0.981 - ETA: 2s - loss: 0.0804 - acc: 0.981 - ETA: 2s - loss: 0.0794 - acc: 0.982 - ETA: 2s - loss: 0.0785 - acc: 0.982 - ETA: 2s - loss: 0.0776 - acc: 0.982 - ETA: 2s - loss: 0.0769 - acc: 0.983 - ETA: 1s - loss: 0.0759 - acc: 0.983 - ETA: 1s - loss: 0.0755 - acc: 0.983 - ETA: 1s - loss: 0.0746 - acc: 0.983 - ETA: 1s - loss: 0.0745 - acc: 0.983 - ETA: 0s - loss: 0.0760 - acc: 0.982 - ETA: 0s - loss: 0.0754 - acc: 0.983 - ETA: 0s - loss: 0.0748 - acc: 0.983 - ETA: 0s - loss: 0.0752 - acc: 0.983 - 14s 4ms/step - loss: 0.0760 - acc: 0.9826 - val_loss: 1.1413 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.70521\n",
      "Epoch 6/10\n",
      "3675/3675 [==============================] - ETA: 12s - loss: 0.0821 - acc: 0.96 - ETA: 12s - loss: 0.0697 - acc: 0.96 - ETA: 12s - loss: 0.0662 - acc: 0.96 - ETA: 12s - loss: 0.0550 - acc: 0.97 - ETA: 11s - loss: 0.0479 - acc: 0.98 - ETA: 11s - loss: 0.0457 - acc: 0.98 - ETA: 11s - loss: 0.0414 - acc: 0.98 - ETA: 11s - loss: 0.0390 - acc: 0.98 - ETA: 11s - loss: 0.0361 - acc: 0.98 - ETA: 10s - loss: 0.0378 - acc: 0.98 - ETA: 10s - loss: 0.0470 - acc: 0.98 - ETA: 10s - loss: 0.0443 - acc: 0.98 - ETA: 10s - loss: 0.0425 - acc: 0.98 - ETA: 9s - loss: 0.0471 - acc: 0.9888 - ETA: 9s - loss: 0.0458 - acc: 0.989 - ETA: 9s - loss: 0.0445 - acc: 0.990 - ETA: 9s - loss: 0.0460 - acc: 0.989 - ETA: 9s - loss: 0.0443 - acc: 0.990 - ETA: 8s - loss: 0.0430 - acc: 0.991 - ETA: 8s - loss: 0.0433 - acc: 0.990 - ETA: 8s - loss: 0.0426 - acc: 0.991 - ETA: 8s - loss: 0.0508 - acc: 0.988 - ETA: 7s - loss: 0.0494 - acc: 0.989 - ETA: 7s - loss: 0.0478 - acc: 0.989 - ETA: 7s - loss: 0.0503 - acc: 0.988 - ETA: 7s - loss: 0.0520 - acc: 0.988 - ETA: 7s - loss: 0.0557 - acc: 0.987 - ETA: 6s - loss: 0.0549 - acc: 0.987 - ETA: 6s - loss: 0.0552 - acc: 0.987 - ETA: 6s - loss: 0.0538 - acc: 0.988 - ETA: 6s - loss: 0.0543 - acc: 0.987 - ETA: 5s - loss: 0.0533 - acc: 0.988 - ETA: 5s - loss: 0.0545 - acc: 0.987 - ETA: 5s - loss: 0.0556 - acc: 0.987 - ETA: 5s - loss: 0.0571 - acc: 0.987 - ETA: 4s - loss: 0.0574 - acc: 0.987 - ETA: 4s - loss: 0.0563 - acc: 0.987 - ETA: 4s - loss: 0.0552 - acc: 0.987 - ETA: 4s - loss: 0.0561 - acc: 0.987 - ETA: 4s - loss: 0.0551 - acc: 0.987 - ETA: 3s - loss: 0.0542 - acc: 0.987 - ETA: 3s - loss: 0.0552 - acc: 0.987 - ETA: 3s - loss: 0.0565 - acc: 0.987 - ETA: 3s - loss: 0.0556 - acc: 0.987 - ETA: 2s - loss: 0.0568 - acc: 0.987 - ETA: 2s - loss: 0.0561 - acc: 0.987 - ETA: 2s - loss: 0.0553 - acc: 0.988 - ETA: 2s - loss: 0.0559 - acc: 0.988 - ETA: 1s - loss: 0.0566 - acc: 0.987 - ETA: 1s - loss: 0.0579 - acc: 0.987 - ETA: 1s - loss: 0.0570 - acc: 0.987 - ETA: 1s - loss: 0.0592 - acc: 0.987 - ETA: 1s - loss: 0.0594 - acc: 0.987 - ETA: 0s - loss: 0.0586 - acc: 0.987 - ETA: 0s - loss: 0.0579 - acc: 0.987 - ETA: 0s - loss: 0.0571 - acc: 0.988 - ETA: 0s - loss: 0.0566 - acc: 0.988 - 13s 4ms/step - loss: 0.0565 - acc: 0.9883 - val_loss: 1.1572 - val_acc: 0.6722\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.70521\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675/3675 [==============================] - ETA: 12s - loss: 0.0280 - acc: 0.98 - ETA: 12s - loss: 0.0207 - acc: 0.99 - ETA: 12s - loss: 0.0162 - acc: 0.99 - ETA: 12s - loss: 0.0306 - acc: 0.99 - ETA: 12s - loss: 0.0291 - acc: 0.99 - ETA: 11s - loss: 0.0264 - acc: 0.99 - ETA: 11s - loss: 0.0317 - acc: 0.99 - ETA: 11s - loss: 0.0359 - acc: 0.99 - ETA: 11s - loss: 0.0349 - acc: 0.98 - ETA: 10s - loss: 0.0330 - acc: 0.99 - ETA: 10s - loss: 0.0319 - acc: 0.99 - ETA: 10s - loss: 0.0385 - acc: 0.98 - ETA: 10s - loss: 0.0412 - acc: 0.98 - ETA: 10s - loss: 0.0411 - acc: 0.98 - ETA: 9s - loss: 0.0400 - acc: 0.9865 - ETA: 9s - loss: 0.0380 - acc: 0.987 - ETA: 9s - loss: 0.0374 - acc: 0.988 - ETA: 9s - loss: 0.0372 - acc: 0.988 - ETA: 8s - loss: 0.0356 - acc: 0.989 - ETA: 8s - loss: 0.0341 - acc: 0.989 - ETA: 8s - loss: 0.0344 - acc: 0.988 - ETA: 8s - loss: 0.0333 - acc: 0.989 - ETA: 7s - loss: 0.0324 - acc: 0.989 - ETA: 7s - loss: 0.0314 - acc: 0.990 - ETA: 7s - loss: 0.0307 - acc: 0.990 - ETA: 7s - loss: 0.0320 - acc: 0.990 - ETA: 7s - loss: 0.0311 - acc: 0.990 - ETA: 6s - loss: 0.0303 - acc: 0.991 - ETA: 6s - loss: 0.0298 - acc: 0.991 - ETA: 6s - loss: 0.0333 - acc: 0.991 - ETA: 6s - loss: 0.0340 - acc: 0.990 - ETA: 5s - loss: 0.0365 - acc: 0.990 - ETA: 5s - loss: 0.0378 - acc: 0.990 - ETA: 5s - loss: 0.0370 - acc: 0.990 - ETA: 5s - loss: 0.0376 - acc: 0.989 - ETA: 4s - loss: 0.0370 - acc: 0.990 - ETA: 4s - loss: 0.0374 - acc: 0.989 - ETA: 4s - loss: 0.0376 - acc: 0.989 - ETA: 4s - loss: 0.0368 - acc: 0.990 - ETA: 4s - loss: 0.0366 - acc: 0.989 - ETA: 3s - loss: 0.0359 - acc: 0.990 - ETA: 3s - loss: 0.0357 - acc: 0.990 - ETA: 3s - loss: 0.0363 - acc: 0.989 - ETA: 3s - loss: 0.0357 - acc: 0.989 - ETA: 2s - loss: 0.0369 - acc: 0.989 - ETA: 2s - loss: 0.0363 - acc: 0.989 - ETA: 2s - loss: 0.0368 - acc: 0.989 - ETA: 2s - loss: 0.0375 - acc: 0.988 - ETA: 1s - loss: 0.0370 - acc: 0.989 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0379 - acc: 0.988 - ETA: 1s - loss: 0.0374 - acc: 0.988 - ETA: 0s - loss: 0.0368 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0379 - acc: 0.988 - ETA: 0s - loss: 0.0373 - acc: 0.989 - 14s 4ms/step - loss: 0.0373 - acc: 0.9891 - val_loss: 1.2086 - val_acc: 0.6798\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.70521\n",
      "Epoch 8/10\n",
      "3675/3675 [==============================] - ETA: 12s - loss: 0.0047 - acc: 1.00 - ETA: 12s - loss: 0.0200 - acc: 0.99 - ETA: 12s - loss: 0.0241 - acc: 0.98 - ETA: 12s - loss: 0.0234 - acc: 0.99 - ETA: 12s - loss: 0.0238 - acc: 0.99 - ETA: 11s - loss: 0.0212 - acc: 0.99 - ETA: 11s - loss: 0.0188 - acc: 0.99 - ETA: 11s - loss: 0.0172 - acc: 0.99 - ETA: 11s - loss: 0.0177 - acc: 0.99 - ETA: 11s - loss: 0.0264 - acc: 0.99 - ETA: 10s - loss: 0.0249 - acc: 0.99 - ETA: 10s - loss: 0.0272 - acc: 0.99 - ETA: 10s - loss: 0.0257 - acc: 0.99 - ETA: 10s - loss: 0.0244 - acc: 0.99 - ETA: 9s - loss: 0.0231 - acc: 0.9938 - ETA: 9s - loss: 0.0223 - acc: 0.994 - ETA: 9s - loss: 0.0232 - acc: 0.993 - ETA: 9s - loss: 0.0223 - acc: 0.993 - ETA: 8s - loss: 0.0250 - acc: 0.992 - ETA: 8s - loss: 0.0241 - acc: 0.993 - ETA: 8s - loss: 0.0237 - acc: 0.993 - ETA: 8s - loss: 0.0257 - acc: 0.992 - ETA: 8s - loss: 0.0250 - acc: 0.993 - ETA: 7s - loss: 0.0243 - acc: 0.993 - ETA: 7s - loss: 0.0242 - acc: 0.993 - ETA: 7s - loss: 0.0238 - acc: 0.993 - ETA: 7s - loss: 0.0231 - acc: 0.993 - ETA: 6s - loss: 0.0240 - acc: 0.993 - ETA: 6s - loss: 0.0238 - acc: 0.993 - ETA: 6s - loss: 0.0233 - acc: 0.993 - ETA: 6s - loss: 0.0229 - acc: 0.993 - ETA: 6s - loss: 0.0228 - acc: 0.993 - ETA: 5s - loss: 0.0234 - acc: 0.993 - ETA: 5s - loss: 0.0229 - acc: 0.993 - ETA: 5s - loss: 0.0256 - acc: 0.993 - ETA: 5s - loss: 0.0260 - acc: 0.993 - ETA: 4s - loss: 0.0255 - acc: 0.993 - ETA: 4s - loss: 0.0249 - acc: 0.993 - ETA: 4s - loss: 0.0277 - acc: 0.992 - ETA: 4s - loss: 0.0277 - acc: 0.993 - ETA: 3s - loss: 0.0306 - acc: 0.992 - ETA: 3s - loss: 0.0301 - acc: 0.992 - ETA: 3s - loss: 0.0297 - acc: 0.992 - ETA: 3s - loss: 0.0303 - acc: 0.992 - ETA: 2s - loss: 0.0303 - acc: 0.992 - ETA: 2s - loss: 0.0298 - acc: 0.992 - ETA: 2s - loss: 0.0292 - acc: 0.992 - ETA: 2s - loss: 0.0291 - acc: 0.992 - ETA: 2s - loss: 0.0288 - acc: 0.992 - ETA: 1s - loss: 0.0306 - acc: 0.991 - ETA: 1s - loss: 0.0320 - acc: 0.991 - ETA: 1s - loss: 0.0319 - acc: 0.991 - ETA: 1s - loss: 0.0315 - acc: 0.991 - ETA: 0s - loss: 0.0312 - acc: 0.991 - ETA: 0s - loss: 0.0318 - acc: 0.990 - ETA: 0s - loss: 0.0316 - acc: 0.991 - ETA: 0s - loss: 0.0321 - acc: 0.990 - 14s 4ms/step - loss: 0.0319 - acc: 0.9907 - val_loss: 1.2817 - val_acc: 0.6633\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.70521\n",
      "Epoch 9/10\n",
      "3675/3675 [==============================] - ETA: 14s - loss: 0.0062 - acc: 1.00 - ETA: 13s - loss: 0.0225 - acc: 0.99 - ETA: 13s - loss: 0.0284 - acc: 0.98 - ETA: 12s - loss: 0.0326 - acc: 0.98 - ETA: 12s - loss: 0.0334 - acc: 0.98 - ETA: 12s - loss: 0.0316 - acc: 0.98 - ETA: 12s - loss: 0.0278 - acc: 0.98 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 11s - loss: 0.0247 - acc: 0.99 - ETA: 11s - loss: 0.0241 - acc: 0.99 - ETA: 11s - loss: 0.0222 - acc: 0.99 - ETA: 11s - loss: 0.0250 - acc: 0.99 - ETA: 10s - loss: 0.0242 - acc: 0.99 - ETA: 10s - loss: 0.0266 - acc: 0.99 - ETA: 10s - loss: 0.0276 - acc: 0.99 - ETA: 10s - loss: 0.0262 - acc: 0.99 - ETA: 9s - loss: 0.0250 - acc: 0.9917 - ETA: 9s - loss: 0.0239 - acc: 0.992 - ETA: 9s - loss: 0.0233 - acc: 0.992 - ETA: 9s - loss: 0.0224 - acc: 0.993 - ETA: 9s - loss: 0.0218 - acc: 0.993 - ETA: 8s - loss: 0.0211 - acc: 0.993 - ETA: 8s - loss: 0.0219 - acc: 0.993 - ETA: 8s - loss: 0.0252 - acc: 0.992 - ETA: 8s - loss: 0.0257 - acc: 0.992 - ETA: 7s - loss: 0.0251 - acc: 0.992 - ETA: 7s - loss: 0.0245 - acc: 0.993 - ETA: 7s - loss: 0.0255 - acc: 0.992 - ETA: 7s - loss: 0.0249 - acc: 0.993 - ETA: 6s - loss: 0.0253 - acc: 0.992 - ETA: 6s - loss: 0.0260 - acc: 0.992 - ETA: 6s - loss: 0.0253 - acc: 0.992 - ETA: 6s - loss: 0.0250 - acc: 0.992 - ETA: 5s - loss: 0.0244 - acc: 0.993 - ETA: 5s - loss: 0.0238 - acc: 0.993 - ETA: 5s - loss: 0.0246 - acc: 0.993 - ETA: 5s - loss: 0.0241 - acc: 0.993 - ETA: 4s - loss: 0.0237 - acc: 0.993 - ETA: 4s - loss: 0.0248 - acc: 0.993 - ETA: 4s - loss: 0.0252 - acc: 0.993 - ETA: 4s - loss: 0.0247 - acc: 0.993 - ETA: 3s - loss: 0.0251 - acc: 0.992 - ETA: 3s - loss: 0.0247 - acc: 0.993 - ETA: 3s - loss: 0.0253 - acc: 0.992 - ETA: 3s - loss: 0.0253 - acc: 0.992 - ETA: 2s - loss: 0.0266 - acc: 0.992 - ETA: 2s - loss: 0.0261 - acc: 0.992 - ETA: 2s - loss: 0.0257 - acc: 0.992 - ETA: 2s - loss: 0.0255 - acc: 0.993 - ETA: 1s - loss: 0.0250 - acc: 0.993 - ETA: 1s - loss: 0.0247 - acc: 0.993 - ETA: 1s - loss: 0.0244 - acc: 0.993 - ETA: 1s - loss: 0.0240 - acc: 0.993 - ETA: 0s - loss: 0.0236 - acc: 0.993 - ETA: 0s - loss: 0.0241 - acc: 0.993 - ETA: 0s - loss: 0.0244 - acc: 0.993 - ETA: 0s - loss: 0.0240 - acc: 0.993 - 14s 4ms/step - loss: 0.0239 - acc: 0.9935 - val_loss: 1.3467 - val_acc: 0.6798\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.70521\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675/3675 [==============================] - ETA: 14s - loss: 0.0051 - acc: 1.00 - ETA: 13s - loss: 0.0039 - acc: 1.00 - ETA: 13s - loss: 0.0121 - acc: 0.99 - ETA: 12s - loss: 0.0101 - acc: 0.99 - ETA: 12s - loss: 0.0262 - acc: 0.99 - ETA: 12s - loss: 0.0222 - acc: 0.99 - ETA: 12s - loss: 0.0194 - acc: 0.99 - ETA: 11s - loss: 0.0229 - acc: 0.99 - ETA: 11s - loss: 0.0207 - acc: 0.99 - ETA: 11s - loss: 0.0195 - acc: 0.99 - ETA: 11s - loss: 0.0186 - acc: 0.99 - ETA: 10s - loss: 0.0226 - acc: 0.99 - ETA: 10s - loss: 0.0303 - acc: 0.99 - ETA: 10s - loss: 0.0310 - acc: 0.99 - ETA: 10s - loss: 0.0316 - acc: 0.98 - ETA: 10s - loss: 0.0312 - acc: 0.98 - ETA: 9s - loss: 0.0299 - acc: 0.9899 - ETA: 9s - loss: 0.0285 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0267 - acc: 0.990 - ETA: 8s - loss: 0.0259 - acc: 0.991 - ETA: 8s - loss: 0.0254 - acc: 0.991 - ETA: 8s - loss: 0.0257 - acc: 0.991 - ETA: 8s - loss: 0.0250 - acc: 0.991 - ETA: 7s - loss: 0.0251 - acc: 0.991 - ETA: 7s - loss: 0.0249 - acc: 0.991 - ETA: 7s - loss: 0.0250 - acc: 0.990 - ETA: 7s - loss: 0.0243 - acc: 0.991 - ETA: 6s - loss: 0.0237 - acc: 0.991 - ETA: 6s - loss: 0.0238 - acc: 0.991 - ETA: 6s - loss: 0.0251 - acc: 0.990 - ETA: 6s - loss: 0.0277 - acc: 0.990 - ETA: 5s - loss: 0.0272 - acc: 0.991 - ETA: 5s - loss: 0.0266 - acc: 0.991 - ETA: 5s - loss: 0.0262 - acc: 0.991 - ETA: 5s - loss: 0.0257 - acc: 0.991 - ETA: 5s - loss: 0.0251 - acc: 0.992 - ETA: 4s - loss: 0.0247 - acc: 0.992 - ETA: 4s - loss: 0.0242 - acc: 0.992 - ETA: 4s - loss: 0.0242 - acc: 0.992 - ETA: 4s - loss: 0.0237 - acc: 0.992 - ETA: 3s - loss: 0.0269 - acc: 0.991 - ETA: 3s - loss: 0.0265 - acc: 0.991 - ETA: 3s - loss: 0.0269 - acc: 0.991 - ETA: 3s - loss: 0.0271 - acc: 0.991 - ETA: 2s - loss: 0.0269 - acc: 0.991 - ETA: 2s - loss: 0.0265 - acc: 0.991 - ETA: 2s - loss: 0.0260 - acc: 0.991 - ETA: 2s - loss: 0.0266 - acc: 0.991 - ETA: 1s - loss: 0.0279 - acc: 0.990 - ETA: 1s - loss: 0.0274 - acc: 0.990 - ETA: 1s - loss: 0.0270 - acc: 0.991 - ETA: 1s - loss: 0.0266 - acc: 0.991 - ETA: 0s - loss: 0.0262 - acc: 0.991 - ETA: 0s - loss: 0.0258 - acc: 0.991 - ETA: 0s - loss: 0.0255 - acc: 0.991 - ETA: 0s - loss: 0.0260 - acc: 0.991 - 14s 4ms/step - loss: 0.0258 - acc: 0.9916 - val_loss: 1.3749 - val_acc: 0.6607\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.70521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x183d7e5f908>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"CNN_best_weights.{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, y_train, batch_size=64, epochs=10,\n",
    "                     validation_data=(x_val_seq, y_validation), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#tvec = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "#tvec.fit(x_train)\n",
    "\n",
    "#x_train_tfidf = tvec.transform(x_train)\n",
    "#x_test_tfidf = tvec.transform(x_test)\n",
    "#lr_with_tfidf = LogisticRegression()\n",
    "#lr_with_tfidf.fit(x_train_tfidf,y_train)\n",
    "#lr_with_tfidf.score(x_test_tfidf,y_test)\n",
    "#yhat_lr = lr_with_tfidf.predict_proba(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_CNN_model = load_model('CNN_best_weights.02-0.71.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_seq = pad_sequences(sequences_test, maxlen=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788/788 [==============================] - ETA:  - ETA:  - ETA:  - 0s 571us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7618807530040064, 0.7017766500487546]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_CNN_model.evaluate(x=x_test_seq, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_cnn = loaded_CNN_model.predict(x_test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DataSet Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Reading Test Dataset\n",
    "def read_data(tweets_data_path):\n",
    "    tweets_data = []\n",
    "    tweets_file = open(tweets_data_path, \"r\")\n",
    "    line = tweets_file.readline()\n",
    "    for line in tweets_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    data = pd.DataFrame.from_dict(tweets_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Test Dataset\n",
    "tweet_data=read_data(\"D:/Thesis/autonomousDriveMulti.json\")\n",
    "tweet_data1=read_data(\"D:/Thesis/autonomousDrive.json\")\n",
    "tweet_data2=read_data(\"D:/Thesis/python.json\")\n",
    "tweets= pd.concat([tweet_data, tweet_data1,tweet_data2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test Dataset Only\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Languages', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 languages', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:5].plot(ax=ax, kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test Dataset Only\n",
    "tweets_by_lang = tweets['source'].value_counts()\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Source', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 languages', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:5].plot(ax=ax, kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_data_path = 'D:/Thesis/autonomousDrive.json'\n",
    "\n",
    "#tweets_data = []\n",
    "#tweets_file = open(tweets_data_path, \"r\")\n",
    "#line = tweets_file.readline()\n",
    "#for line in tweets_file:\n",
    "#    try:\n",
    "#        tweet = json.loads(line)\n",
    "#        tweets_data.append(tweet)\n",
    "#    except:\n",
    "#        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = pd.DataFrame.from_dict(tweets_data)\n",
    "#tweets = pd.read_excel(\"D:\\\\Thesis\\\\Code\\\\tweet.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion to xlsx (optional)\n",
    "writer = ExcelWriter('tweet.xlsx')\n",
    "tweets.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test Dataset Only\n",
    "tweets['created_at'] = pd.to_datetime(tweets.created_at)\n",
    "tweets['source'] = tweets['source'].str.extract('>(.+?)<', expand=False).str.strip()\n",
    "tweets['pre_clean_len'] = [len(t) for t in tweets.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test Dataset Only\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(tweets.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data Clean\n",
    "x = len(tweets.index)\n",
    "nums = [0,x]\n",
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    if( (i+1)%881 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                    \n",
    "    clean_tweet_texts.append(tweet_cleaner_updated(tweets['text'][i]))\n",
    "    clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(message for message in tweets.text)\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(width = 2400, height = 1600).generate(text)\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='gaussian')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_len=pd.Series(data=tweets['pre_clean_len'].values, index=tweets['created_at'])\n",
    "time_len.plot(figsize=(16, 4), color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df.groupby(level=0).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
